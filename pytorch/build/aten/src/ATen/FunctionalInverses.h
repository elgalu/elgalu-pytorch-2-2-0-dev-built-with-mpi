#pragma once

// @generated by torchgen/gen.py from FunctionalInverses.h

#include <ATen/Tensor.h>

namespace at {
namespace functionalization {

struct FunctionalInverses {

static at::Tensor _fw_primal_copy_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views, int64_t level);
static at::Tensor _make_dual_copy_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views, const at::Tensor & tangent, int64_t level);
static at::Tensor view_as_real_copy_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views);
static at::Tensor view_as_complex_copy_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views);
static at::Tensor _conj_copy_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views);
static at::Tensor _neg_view_copy_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views);
static at::Tensor as_strided_copy_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, c10::optional<c10::SymInt> storage_offset);
static at::Tensor _sparse_broadcast_to_copy_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views, at::IntArrayRef size);
static at::Tensor diagonal_copy_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views, int64_t offset, int64_t dim1, int64_t dim2);
static at::Tensor expand_copy_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views, c10::SymIntArrayRef size, bool implicit);
static at::Tensor permute_copy_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views, at::IntArrayRef dims);
static at::Tensor _reshape_alias_copy_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views, c10::SymIntArrayRef size, c10::SymIntArrayRef stride);
static at::Tensor select_copy_int_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views, int64_t dim, c10::SymInt index);
static at::Tensor detach_copy_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views);
static at::Tensor slice_copy_Tensor_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views, int64_t dim, c10::optional<c10::SymInt> start, c10::optional<c10::SymInt> end, c10::SymInt step);
static at::Tensor split_copy_Tensor_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views, int64_t mutated_view_idx, c10::SymInt split_size, int64_t dim);
static at::Tensor split_with_sizes_copy_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views, int64_t mutated_view_idx, c10::SymIntArrayRef split_sizes, int64_t dim);
static at::Tensor squeeze_copy_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views);
static at::Tensor squeeze_copy_dim_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views, int64_t dim);
static at::Tensor squeeze_copy_dims_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views, at::IntArrayRef dim);
static at::Tensor t_copy_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views);
static at::Tensor transpose_copy_int_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views, int64_t dim0, int64_t dim1);
static at::Tensor _nested_view_from_buffer_copy_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views, const at::Tensor & nested_size, const at::Tensor & nested_strides, const at::Tensor & offsets);
static at::Tensor unsqueeze_copy_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views, int64_t dim);
static at::Tensor _indices_copy_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views);
static at::Tensor _values_copy_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views);
static at::Tensor indices_copy_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views);
static at::Tensor values_copy_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views);
static at::Tensor crow_indices_copy_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views);
static at::Tensor col_indices_copy_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views);
static at::Tensor ccol_indices_copy_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views);
static at::Tensor row_indices_copy_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views);
static at::Tensor unbind_copy_int_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views, int64_t mutated_view_idx, int64_t dim);
static at::Tensor lift_fresh_copy_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views);
static at::Tensor view_copy_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views, c10::SymIntArrayRef size);
static at::Tensor view_copy_dtype_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views, at::ScalarType dtype);
static at::Tensor unfold_copy_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views, int64_t dimension, int64_t size, int64_t step);
static at::Tensor alias_copy_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views);
static at::Tensor _test_autograd_multiple_dispatch_view_copy_inverse(const at::Tensor & base, const at::Tensor & mutated_view, bool reapply_views);

};
}
}
